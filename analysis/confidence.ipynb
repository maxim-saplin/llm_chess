{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Confidence vs Performance Analysis\n",
                "\n",
                "This notebook analyzes the \"Self-confidence index\" of chess agents and correlates it with their performance (Elo, Game Duration).\n",
                "\n",
                "## Metrics Definition\n",
                "- **Self-confidence index**: Defined as the ratio of moves made without requesting `get_legal_moves`. Calculated as `get_legal_moves_count / make_move_count`.\n",
                "- **Confidence Score**: `1 - min(1.0, confidence_ratio)`. Higher score means the model requests legal moves less often relative to making moves.\n",
                "- **Performance**: Elo ratings (from `elo_refined.csv`) and Game Duration (from logs)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "import os\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import numpy as np\n",
                "from collections import defaultdict\n",
                "\n",
                "# Add project root to path to allow importing from data_processing\n",
                "# Assuming notebook is in analysis/ folder, project root is one level up\n",
                "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
                "if project_root not in sys.path:\n",
                "    sys.path.append(project_root)\n",
                "\n",
                "from data_processing.get_refined_csv import (\n",
                "    load_game_logs, \n",
                "    GameMode, \n",
                "    MODEL_OVERRIDES, \n",
                "    ALIASES, \n",
                "    FILTER_OUT_MODELS,\n",
                "    MODELS_METADATA_CSV\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Data Loading\n",
                "\n",
                "We load game logs from both Random-vs-LLM and Dragon-vs-LLM modes. \n",
                "**Crucially**, we exclude `_logs/_pre_aug_2025/no_reflection` as requested, because those logs contain invalid action stats."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define directories, explicitly EXCLUDING \"_logs/_pre_aug_2025/no_reflection\"\n",
                "RANDOM_LOGS_DIRS = [\n",
                "    os.path.join(project_root, \"_logs/rand_vs_llm\"),\n",
                "    os.path.join(project_root, \"_logs/_pre_aug_2025/new\"),\n",
                "    # \"_logs/_pre_aug_2025/no_reflection\"  <-- EXCLUDED\n",
                "]\n",
                "\n",
                "ENGINE_LOGS_DIRS = [\n",
                "    os.path.join(project_root, \"_logs/engine_vs_llm\"),\n",
                "    os.path.join(project_root, \"_logs/_pre_aug_2025/dragon_vs_llm\"),\n",
                "]\n",
                "\n",
                "print(\"Loading Random-vs-LLM logs...\")\n",
                "random_logs = load_game_logs(\n",
                "    logs_dirs=RANDOM_LOGS_DIRS, \n",
                "    model_overrides=MODEL_OVERRIDES, \n",
                "    mode=GameMode.RANDOM_VS_LLM\n",
                ")\n",
                "\n",
                "print(\"Loading Dragon-vs-LLM logs...\")\n",
                "dragon_logs = load_game_logs(\n",
                "    logs_dirs=ENGINE_LOGS_DIRS, \n",
                "    model_overrides=MODEL_OVERRIDES, \n",
                "    mode=GameMode.DRAGON_VS_LLM\n",
                ")\n",
                "\n",
                "all_logs = random_logs + dragon_logs\n",
                "print(f\"Total games loaded: {len(all_logs)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Feature Extraction\n",
                "\n",
                "We aggregate `get_legal_moves_count` and `make_move_count` per model to calculate the confidence metrics."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model_stats = defaultdict(lambda: {\n",
                "    \"total_games\": 0, \n",
                "    \"get_legal_moves\": 0, \n",
                "    \"make_moves\": 0,\n",
                "    \"total_game_duration\": 0.0\n",
                "})\n",
                "\n",
                "for log in all_logs:\n",
                "    model_name = log.player_black.model\n",
                "    # Apply aliases\n",
                "    model_name = ALIASES.get(model_name, model_name)\n",
                "    \n",
                "    if model_name in FILTER_OUT_MODELS:\n",
                "        continue\n",
                "        \n",
                "    stats = model_stats[model_name]\n",
                "    stats[\"total_games\"] += 1\n",
                "    \n",
                "    # Handle missing counts gracefully (-1 indicates missing in some contexts, but usually 0 if not present)\n",
                "    # In the provided logs example, these fields are integers.\n",
                "    glm = log.player_black.get_legal_moves_count\n",
                "    mm = log.player_black.make_move_count\n",
                "    \n",
                "    if glm >= 0:\n",
                "        stats[\"get_legal_moves\"] += glm\n",
                "    if mm >= 0:\n",
                "        stats[\"make_moves\"] += mm\n",
                "        \n",
                "    stats[\"total_game_duration\"] += log.game_duration\n",
                "\n",
                "# Convert to DataFrame\n",
                "data = []\n",
                "for model, stats in model_stats.items():\n",
                "    if stats[\"total_games\"] < 10:  # Filter out models with very few games for stability\n",
                "        continue\n",
                "        \n",
                "    make_moves = stats[\"make_moves\"]\n",
                "    get_legal = stats[\"get_legal_moves\"]\n",
                "    \n",
                "    # Avoid division by zero\n",
                "    if make_moves > 0:\n",
                "        ratio = get_legal / make_moves\n",
                "        # Confidence Score: 1 - ratio (clamped at 0)\n",
                "        # If ratio > 1 (checking multiple times per move), confidence score goes to 0\n",
                "        confidence_score = 1.0 - min(1.0, ratio)\n",
                "    else:\n",
                "        ratio = np.nan\n",
                "        confidence_score = np.nan\n",
                "        \n",
                "    avg_duration = stats[\"total_game_duration\"] / stats[\"total_games\"]\n",
                "    \n",
                "    data.append({\n",
                "        \"Player\": model,\n",
                "        \"Games\": stats[\"total_games\"],\n",
                "        \"Get_Legal_Moves\": get_legal,\n",
                "        \"Make_Moves\": make_moves,\n",
                "        \"Legal_Moves_per_Move_Ratio\": ratio,\n",
                "        \"Confidence_Score\": confidence_score,\n",
                "        \"Avg_Game_Duration\": avg_duration\n",
                "    })\n",
                "\n",
                "df_confidence = pd.DataFrame(data)\n",
                "df_confidence.sort_values(\"Confidence_Score\", ascending=False, inplace=True)\n",
                "df_confidence.head(10)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Merge with Elo Data\n",
                "\n",
                "We import the calculated Elo ratings from `elo_refined.csv`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "elo_file = os.path.join(project_root, \"data_processing/elo_refined.csv\")\n",
                "df_elo = pd.read_csv(elo_file)\n",
                "\n",
                "# Ensure Elo is numeric\n",
                "df_elo[\"elo\"] = pd.to_numeric(df_elo[\"elo\"], errors=\"coerce\")\n",
                "\n",
                "# Merge\n",
                "df_merged = pd.merge(df_confidence, df_elo[[\"Player\", \"elo\", \"win_loss\", \"player_wins_percent\"]], on=\"Player\", how=\"inner\")\n",
                "\n",
                "# Drop rows without Elo if analyzing Elo correlation\n",
                "df_elo_analysis = df_merged.dropna(subset=[\"elo\"])\n",
                "\n",
                "print(f\"Models with Confidence Stats: {len(df_confidence)}\")\n",
                "print(f\"Models with Elo (matched): {len(df_elo_analysis)}\")\n",
                "df_merged.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Analysis & Visualization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "sns.set_theme(style=\"whitegrid\")\n",
                "\n",
                "# 1. Distribution of Confidence Scores\n",
                "plt.figure(figsize=(10, 6))\n",
                "sns.histplot(df_confidence[\"Confidence_Score\"], bins=20, kde=True)\n",
                "plt.title(\"Distribution of Self-Confidence Scores\")\n",
                "plt.xlabel(\"Confidence Score (1 - LegalRequests/Moves)\")\n",
                "plt.ylabel(\"Count of Models\")\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2. Confidence vs Elo\n",
                "plt.figure(figsize=(12, 8))\n",
                "sns.scatterplot(data=df_elo_analysis, x=\"Confidence_Score\", y=\"elo\", size=\"Games\", sizes=(20, 200), alpha=0.7)\n",
                "\n",
                "# Add regression line\n",
                "sns.regplot(data=df_elo_analysis, x=\"Confidence_Score\", y=\"elo\", scatter=False, color=\"red\")\n",
                "\n",
                "# Label top models\n",
                "for i, row in df_elo_analysis.iterrows():\n",
                "    if row[\"elo\"] > 2500 or row[\"Confidence_Score\"] > 0.95 or row[\"Confidence_Score\"] < 0.1:\n",
                "        plt.text(row[\"Confidence_Score\"]+0.01, row[\"elo\"], row[\"Player\"], fontsize=8, alpha=0.7)\n",
                "\n",
                "plt.title(\"Model Performance (Elo) vs Self-Confidence\")\n",
                "plt.xlabel(\"Confidence Score (Higher = Fewer Legal Move Checks)\")\n",
                "plt.ylabel(\"Elo Rating\")\n",
                "plt.show()\n",
                "\n",
                "correlation = df_elo_analysis[\"Confidence_Score\"].corr(df_elo_analysis[\"elo\"])\n",
                "print(f\"Correlation between Confidence and Elo: {correlation:.3f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3. Confidence vs Game Duration\n",
                "plt.figure(figsize=(12, 8))\n",
                "sns.scatterplot(data=df_merged, x=\"Confidence_Score\", y=\"Avg_Game_Duration\", size=\"Games\", sizes=(20, 200), alpha=0.7)\n",
                "sns.regplot(data=df_merged, x=\"Confidence_Score\", y=\"Avg_Game_Duration\", scatter=False, color=\"green\")\n",
                "\n",
                "plt.title(\"Game Duration vs Self-Confidence\")\n",
                "plt.xlabel(\"Confidence Score\")\n",
                "plt.ylabel(\"Average Game Duration (Normalized 0-1)\")\n",
                "plt.show()\n",
                "\n",
                "corr_duration = df_merged[\"Confidence_Score\"].corr(df_merged[\"Avg_Game_Duration\"])\n",
                "print(f\"Correlation between Confidence and Game Duration: {corr_duration:.3f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 4. Detailed Correlation Matrix\n",
                "cols = [\"elo\", \"Confidence_Score\", \"Legal_Moves_per_Move_Ratio\", \"Avg_Game_Duration\", \"win_loss\", \"player_wins_percent\"]\n",
                "corr_matrix = df_merged[cols].corr()\n",
                "\n",
                "plt.figure(figsize=(10, 8))\n",
                "sns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
                "plt.title(\"Correlation Matrix of Metrics\")\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Confidence & Elo Table\n",
                "\n",
                "Below is a summary table of models sorted by highest Self-Confidence."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Prepare simple table with relevant columns\n",
                "df_table = df_elo_analysis[[\"Player\", \"Confidence_Score\", \"elo\"]].copy()\n",
                "\n",
                "# Sort descending by Confidence Score\n",
                "df_table.sort_values(by=\"Confidence_Score\", ascending=False, inplace=True)\n",
                "\n",
                "# Reset index for display\n",
                "df_table.reset_index(drop=True, inplace=True)\n",
                "\n",
                "# Display full table (or head/tail if desired, but user asked for \"a simple table\")\n",
                "print(\"Models sorted by Self-Confidence (Highest to Lowest):\\n\")\n",
                "print(df_table.to_string(index=True, float_format=\"{:.3f}\".format))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Findings Summary\n",
                "\n",
                "1. **Confidence Metric**: We defined confidence as the inverse of how often the model checks for legal moves (`1 - get_legal/make_moves`). A score of 1.0 means the model never checks legal moves.\n",
                "2. **Distribution**: The histogram shows how models vary in their reliance on the legal moves tool.\n",
                "3. **Performance Link**: The scatter plots reveal whether \"confident\" models (those that don't check moves) tend to have higher Elo or last longer in games.\n",
                "4. **Summary Table**: The table above lists each model's confidence score alongside its Elo, sorted by confidence."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}