{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas Jinja2 matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "from aggregate_logs_to_csv import aggregate_models_to_csv, MODEL_OVERRIDES\n",
    "from aggr_logs_to_plain_csv import aggregate_logs_to_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Aggregate CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOGS_DIR = \"_logs/no_reflection\"\n",
    "AGGREGATE_CSV = os.path.join(LOGS_DIR, \"aggregate_models.csv\")\n",
    "REFINED_CSV = \"data_processing/refined.csv\"\n",
    "\n",
    "aggregate_models_to_csv(\"../_logs/no_reflection\",\"aggr.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            model_name  total_games  black_llm_wins  white_rand_wins  draws  black_llm_wins_percent  black_llm_draws_percent  llm_total_moves  llm_wrong_actions  llm_wrong_moves  llm_avg_material  llm_std_dev_material  rand_avg_material  rand_std_dev_material  material_diff_llm_minus_rand  material_diff_llm_minus_rand_per_100moves  wrong_actions_per_100moves  wrong_moves_per_100moves  wrong_actions_per_1000moves  wrong_moves_per_1000moves  mistakes_per_1000moves  std_dev_wrong_actions_per_1000moves  std_dev_wrong_moves_per_1000moves  std_dev_mistakes_per_1000moves  average_moves  std_dev_moves  completion_tokens_black  completion_tokens_black_per_move  min_moves  max_moves  prompt_tokens_black  total_tokens_black  moe_material_diff  moe_avg_moves  moe_wrong_actions_per_1000moves  moe_wrong_moves_per_1000moves  moe_mistakes_per_1000moves\n",
      "          anthropic.claude-v3-5-sonnet           30               0                8     22                0.000000                73.333333             5148                  0               12         15.800000              9.918287          22.500000              11.840171                     -6.700000                                  -3.695372                    0.000000                  1.465774                     0.000000                  14.657738               14.657738                             0.000000                          55.735992                       55.735992     171.600000      55.148452                   417373                         81.074786         10        200              2997738             3415111           3.674824      19.734620                         0.000000                      19.944868                   19.944868\n",
      "       anthropic.claude-v3-5-sonnet-v1           60               4                8     48                6.666667                80.000000            11003                  0                3         13.200000              8.297028          17.366667               9.425833                     -4.166667                                  -1.932540                    0.000000                  0.166667                     0.000000                   1.666667                1.666667                             0.000000                          12.909944                       12.909944     183.383333      40.117662                   884906                         80.424066         30        200              6420741             7305647           3.004864      10.151169                         0.000000                       3.266667                    3.266667\n",
      "       anthropic.claude-v3-5-sonnet-v2           60               2                5     53                3.333333                88.333333            11292                  0                8         10.816667              7.740476          13.833333               8.843587                     -3.016667                                  -0.511342                    0.000000                  0.298287                     0.000000                   2.982872                2.982872                             0.000000                          18.027529                       18.027529     188.200000      37.124984                  1025929                         90.854499         22        200              6510330             7536259           2.559979       9.393917                         0.000000                       4.561594                    4.561594\n",
      "             anthropic.claude-v3-haiku           40               0               40      0                0.000000                 0.000000             1334                  7                4         36.725000              3.389180          37.150000               3.731708                     -0.425000                                  -1.171085                    1.320346                  1.354167                    13.203463                  13.541667               26.745130                            56.506168                          64.264619                       99.020848      33.350000      25.662054                   280994                        210.640180          2         98              2052970             2333964           1.073278       7.952753                        17.511443                      19.915812                   30.686879\n",
      "              anthropic.claude-v3-opus           30               0                5     25                0.000000                83.333333             4968                  1                7         15.633333             10.479811          21.633333              10.189526                     -6.000000                                  -3.349804                    0.185185                  0.745763                     1.851852                   7.457635                9.309486                            10.143010                          26.144328                       34.402708     165.600000      66.459010                   361980                         72.862319         18        200              2833235             3195215           3.159008      23.782051                         3.629630                       9.355628                   12.310851\n",
      "                         deepseek-chat           70               0               68      2                0.000000                 2.857143             4043                 10              180         32.171429              9.020060          30.528571               8.534196                      1.642857                                   3.527257                    0.197791                  9.634065                     1.977915                  96.340648               98.318562                             8.077225                          89.778101                       88.872150      57.757143      50.628638                   998322                        246.926045          8        200              2614032             3612354           1.424718      11.860508                         1.892210                      21.031850                   20.819617\n",
      "     deepseek-r1-distill-qwen-14b@q8_0           30               0               30      0                0.000000                 0.000000               78                 82                8         39.000000              0.000000          39.000000               0.000000                      0.000000                                   0.000000                  117.500000                 11.666667                  1175.000000                 116.666667             1291.666667                           460.275405                         333.045853                      354.061005       2.600000       1.069966                   239699                       3073.064103          2          6               175575              415274           0.000000       0.382882                       164.707438                     119.178928                  126.699104\n",
      "   deepseek-r1-distill-qwen-32b@q4_k_m          108               0              107      1                0.000000                 0.925926              377                246               49         39.000000              0.000000          38.990741               0.096225                      0.009259                                   0.092593                   83.294753                 13.171296                   832.947531                 131.712963              964.660494                           522.685503                         259.538837                      469.866741       3.490741       1.886971                   820049                       2175.196286          1         10               720085             1540134           0.018148       0.355885                        98.579055                      48.949307                   88.617378\n",
      "                     deepseek-reasoner           31               7               18      6               22.580645                19.354839             2845                  9               44         21.935484             13.971245          10.806452              14.005759                     11.129032                                  17.038444                    0.610575                  4.682528                     6.105747                  46.825283               52.931029                            21.376097                          91.350847                       93.189127      91.774194      65.017285                 13044247                       4584.972583          4        200               901267            13945514           3.603464      22.887800                         7.524950                      32.157909                   32.805032\n",
      "                  gemini-1.5-flash-001           30               0               20     10                0.000000                33.333333             2524                 18               36         29.266667             12.119899          35.666667               6.047703                     -6.400000                                  -3.314049                   11.457672                 22.915344                   114.576720                 229.153439              343.730159                           112.800075                         225.600151                      338.400226      84.133333      93.697508                    50265                         19.914818          4        200              1234960             1285225           3.127540      33.529223                        40.364989                      80.729977                  121.094966\n",
      "           gemini-1.5-pro-preview-0409           40               0               37      3                0.000000                 7.500000             2626                 14               71         29.750000              9.662908          33.075000               7.169692                     -3.325000                                  -3.836997                    0.473273                  7.771718                     4.732730                  77.717178               82.449908                             8.100324                          76.131345                       72.662878      65.650000      59.733833                    35145                         13.383473          5        200              1665689             1700834           2.355335      18.511707                         2.510316                      23.593348                   22.518459\n",
      "                  gemini-2.0-flash-exp           30               0               28      2                0.000000                 6.666667             2576                  0               81         26.933333             12.250076          27.166667              12.512293                     -0.233333                                  -1.219356                    0.000000                 11.739474                     0.000000                 117.394744              117.394744                             0.000000                         131.059012                      131.059012      85.866667      73.623335                   433155                        168.150233          6        200              1720039             2153194           1.881101      26.345772                         0.000000                      46.898865                   46.898865\n",
      "   gemini-2.0-flash-thinking-exp-01-21           33               0               33      0                0.000000                 0.000000             1341                  7                1         33.212121              6.989982          33.727273               6.969903                     -0.515152                                  -0.768642                    4.962982                  0.061843                    49.629821                   0.618429               50.248250                           260.958497                           3.552605                      260.861371      40.636364      25.123219                    23829                         17.769575          2        119               625305              649134           1.841571       8.571847                        89.037005                       1.212121                   89.003867\n",
      "    gemini-2.0-flash-thinking-exp-1219           30               0               30      0                0.000000                 0.000000               70                 89                1         39.000000              0.000000          39.000000               0.000000                      0.000000                                   0.000000                  138.611111                  0.555556                  1386.111111                   5.555556             1391.666667                           302.830938                          30.429031                      283.780918       2.333333       0.922266                    50718                        724.542857          2          6               224271              274989           0.000000       0.330029                       108.366660                      10.888889                  101.549697\n",
      "                 gemma-2-27b-it@q6_k_l           30               0               22      8                0.000000                26.666667             3268                  8               52         25.466667             11.717165          26.333333              10.306788                     -0.866667                                   1.339260                    0.383141                  4.807826                     3.831408                  48.078261               51.909669                             8.092186                          69.585084                       69.635666     108.933333      70.564196                   179885                         55.044370         10        200              2028385             2208270           2.669171      25.251073                         2.895751                      24.900703                   24.918803\n",
      "                    gemma-2-9b-it-8bit           30               0               26      4                0.000000                13.333333             2075                 45               31         32.000000              9.180189          34.133333               7.758125                     -2.133333                                  -3.055976                   16.366923                  7.124054                   163.669227                  71.240543              234.909770                           372.225541                         113.887523                      378.257458      69.166667      72.833340                   120608                         58.124337          2        200              1201066             1321674           1.804545      26.063076                       133.199199                      40.754127                  135.357693\n",
      "                          gemma2-9b-it           35               0               35      0                0.000000                 0.000000              516                 83               22         38.714286              1.045197          38.314286               2.272229                      0.400000                                   1.161304                   41.480914                  6.174549                   414.809142                  61.745489              476.554631                           479.257861                          85.575010                      463.030923      14.742857      15.897360                    10432                         20.217054          2         88               342837              353269           0.563612       5.266803                       158.778354                      28.351041                  153.402361\n",
      "                     gpt-35-turbo-0125           30               0               30      0                0.000000                 0.000000               86                 90                0         39.000000              0.000000          39.000000               0.000000                      0.000000                                   0.000000                  128.500000                  0.000000                  1285.000000                   0.000000             1285.000000                           404.319351                           0.000000                      404.319351       2.866667       1.870521                     7054                         82.023256          2         10                89137               96191           0.000000       0.669358                       144.683822                       0.000000                  144.683822\n",
      "                     gpt-35-turbo-0301           30               0               30      0                0.000000                 0.000000               68                 90                0         39.000000              0.000000          39.000000               0.000000                      0.000000                                   0.000000                  140.000000                  0.000000                  1400.000000                   0.000000             1400.000000                           259.309428                           0.000000                      259.309428       2.266667       0.691492                     4560                         67.058824          2          4                57543               62103           0.000000       0.247447                        92.792687                       0.000000                   92.792687\n",
      "                     gpt-35-turbo-0613           30               0               30      0                0.000000                 0.000000              124                 90                0         39.000000              0.000000          38.966667               0.182574                      0.033333                                   0.277778                  105.166667                  0.000000                  1051.666667                   0.000000             1051.666667                           527.393286                           0.000000                      527.393286       4.133333       2.825174                    11610                         93.629032          2         12               129031              140641           0.065333       1.010976                       188.725264                       0.000000                  188.725264\n",
      "                     gpt-35-turbo-1106           30               0               30      0                0.000000                 0.000000              108                 88                2         39.000000              0.000000          39.000000               0.000000                      0.000000                                   0.000000                  113.027778                  0.888889                  1130.277778                   8.888889             1139.166667                           506.327224                          34.942025                      491.947368       3.600000       2.429701                     5219                         48.324074          2         10                49156               54375           0.000000       0.869457                       181.186870                      12.503843                  176.041105\n",
      "                            gpt-4-0613           17               0                2     15                0.000000                88.235294             3232                  0                0         14.764706              9.437488          20.411765               7.211612                     -5.647059                                  -3.106663                    0.000000                  0.000000                     0.000000                   0.000000                0.000000                             0.000000                           0.000000                        0.000000     190.117647      39.979498                    21268                          6.580446         35        200               548490              569758           4.433767      19.005047                         0.000000                       0.000000                    0.000000\n",
      "                        gpt-4-32k-0613           11               0                1     10                0.000000                90.909091             2157                  0                0         12.909091              7.272614          17.818182               6.193839                     -4.909091                                  -2.715981                    0.000000                  0.000000                     0.000000                   0.000000                0.000000                             0.000000                           0.000000                        0.000000     196.090909      12.964988                    14164                          6.566528        157        200               369433              383597           5.275506       7.661818                         0.000000                       0.000000                    0.000000\n",
      "                gpt-4-turbo-2024-04-09           30               0                2     28                0.000000                93.333333             5786                  0                0         15.433333              8.033050          20.800000               9.654015                     -5.366667                                  -3.062648                    0.000000                  0.000000                     0.000000                   0.000000                0.000000                             0.000000                           0.000000                        0.000000     192.866667      29.930648                    34906                          6.032838         45        200              2439800             2474706           3.719412      10.710545                         0.000000                       0.000000                    0.000000\n",
      "                     gpt-4o-2024-05-13           60               0               12     48                0.000000                80.000000            11057                  1               16         13.633333              7.955313          17.350000               7.049041                     -3.716667                                  -1.527760                    0.008333                  0.346151                     0.083333                   3.461511                3.544844                             0.645497                          13.806160                       13.800001     184.283333      37.280927                   346468                         31.334720         32        200              5019535             5366003           1.914112       9.433376                         0.163333                       3.493441                    3.491882\n",
      "                     gpt-4o-2024-08-06           60               1                9     50                1.666667                83.333333            11214                  0                1         14.483333              7.349603          19.333333               8.711315                     -4.850000                                  -3.206580                    0.000000                  0.015723                     0.000000                   0.157233                0.157233                             0.000000                           1.217919                        1.217919     186.900000      31.814451                    86384                          7.703228         51        200              4649382             4735766           2.569851       8.050167                         0.000000                       0.308176                    0.308176\n",
      "                     gpt-4o-2024-11-20           71               3                6     62                4.225352                87.323944            13470                  1                1         11.901408              8.055442          19.901408               8.637385                     -8.000000                                  -4.177047                    0.007042                  0.007042                     0.070423                   0.070423                0.140845                             0.593391                           0.593391                        0.833166     189.718310      32.637482                   681249                         50.575278         29        200              6229300             6910549           2.499409       7.591779                         0.138028                       0.138028                    0.193802\n",
      "                gpt-4o-mini-2024-07-18           30               0               12     18                0.000000                60.000000             4481                 13               24         20.433333              9.339140          24.866667               9.179187                     -4.433333                                  -3.865782                    0.470145                  3.108335                     4.701452                  31.083346               35.784798                            11.075969                          77.402457                       77.223721     149.366667      70.243165                   484917                        108.216246         10        200              2438163             2923080           3.093589      25.136194                         3.963485                      27.698114                   27.634154\n",
      "               granite-3.1-8b-instruct           30               0               30      0                0.000000                 0.000000              126                 47               12         38.966667              0.182574          39.000000               0.000000                     -0.033333                                  -0.416667                   44.750000                  9.166667                   447.500000                  91.666667              539.166667                           471.811490                         223.553246                      490.543476       4.200000       2.483046                    59110                        469.126984          2         10               377120              436230           0.065333       0.888547                       168.835573                      79.997502                  175.538729\n",
      "                           grok-2-1212           49               2               32     15                4.081633                30.612245             5593                  1               93         25.326531             11.336497          22.387755               9.677759                      2.938776                                   4.224505                    0.051020                  3.228055                     0.510204                  32.280546               32.790750                             3.571429                          33.294808                       33.760289     114.142857      67.887161                   370417                         66.228679         26        200              2811967             3182384           2.520265      19.008405                         1.000000                       9.322546                    9.452881\n",
      "                 internlm3-8b-instruct           30               0               30      0                0.000000                 0.000000              108                 15               39         39.000000              0.000000          39.000000               0.000000                      0.000000                                   0.000000                   16.666667                 41.250000                   166.666667                 412.500000              579.166667                           401.147779                         538.546495                      593.139053       3.600000       1.922642                   166741                       1543.898148          2          8               690605              857346           0.000000       0.688009                       143.548889                     192.716388                  212.252084\n",
      "                       llama-2-7b-chat           30               0               30      0                0.000000                 0.000000               64                 88                2         39.000000              0.000000          39.000000               0.000000                      0.000000                                   0.000000                  142.500000                  2.500000                  1425.000000                  25.000000             1450.000000                           238.077791                         100.644475                      190.281099       2.133333       0.507416                     7444                        116.312500          2          4                75064               82508           0.000000       0.181577                        85.195043                      36.015163                   68.091217\n",
      "              llama-3-70b-instruct-awq           30               0               15     15                0.000000                50.000000             4449                  2               41         21.566667             10.129109          26.966667               7.716902                     -5.400000                                  -2.185616                    0.060606                  2.227814                     0.606061                  22.278143               22.884204                             3.319531                          41.243450                       41.176519     148.300000      62.428525                   185118                         41.608901         14        200              2349787             2534905           3.086325      22.339761                         1.187879                      14.758779                   14.734828\n",
      "                  llama-3.1-8b-instant           60               0               60      0                0.000000                 0.000000             1754                 27              146         36.616667              4.329997          37.533333               2.977126                     -0.916667                                  -2.980310                    3.076102                 14.210738                    30.761016                 142.107380              172.868397                            57.328862                         111.870944                      134.394974      29.233333      21.588106                   292050                        166.505131          6         98              1635437             1927487           0.980876       5.462545                        14.506204                      28.307254                   34.006620\n",
      "          llama-3.2-90b-vision-preview            7               0                5      2                0.000000                28.571429              730                  1               14         24.142857             13.643418          32.428571               9.795529                     -8.285714                                  -5.466511                    0.097847                 12.793857                     0.978474                 127.938568              128.917041                             2.588798                         195.528622                      194.877711     104.285714      82.900974                    24475                         33.527397          6        200               399909              424384           7.707209      61.413901                         1.917808                     144.849630                  144.367429\n",
      "                         llama-3.3-70b           42               0               38      4                0.000000                 9.523810             2886                  2              100         30.904762             11.604897          33.452381               8.121643                     -2.547619                                  -1.265137                    0.330688                 10.734968                     3.306878                 107.349677              110.656556                            15.279171                         127.192508                      129.084107      68.714286      69.413320                   297205                        102.981635          4        200              2032359             2329564           1.745939      20.992987                         4.620949                      38.467411                   39.039496\n",
      "               llama-3.3-70b-versatile            2               0                2      0                0.000000                 0.000000               60                  0                6         37.000000              2.828427          39.000000               0.000000                     -2.000000                                  -5.000000                    0.000000                 11.250000                     0.000000                 112.500000              112.500000                             0.000000                          53.033009                       53.033009      30.000000      14.142136                     7018                        116.966667         20         40                43372               50390           3.920000      19.600000                         0.000000                      73.500000                   73.500000\n",
      "                        llama3-8b-8192           60               0               60      0                0.000000                 0.000000              902                104               66         38.200000              2.097618          38.683333               1.065510                     -0.483333                                  -1.416967                   20.694153                  8.971030                   206.941531                  89.710304              296.651835                           220.415794                          93.519498                      208.911913      15.033333      10.541229                    51432                         57.019956          4         50               749595              801027           0.552198       2.667299                        55.772891                      23.663698                   52.862007\n",
      "                           llama3.1-8b           90               0               87      3                0.000000                 3.333333             2436                 44              188         37.200000              3.736248          37.833333               2.545099                     -0.633333                                  -1.574541                    3.503988                 12.664664                    35.039880                 126.646638              161.686518                            55.908533                         113.698417                      127.446990      27.066667      22.241170                   394879                        162.101396          4        100              1995033             2389912           0.803128       4.595073                        11.550822                      23.490336                   26.330821\n",
      "       meta-llama-3.1-8b-instruct-fp16           30               0               30      0                0.000000                 0.000000              830                 19               61         36.733333              4.093168          37.000000               3.393706                     -0.266667                                   0.463016                    7.235502                 13.547422                    72.355016                 135.474221              207.829237                           158.436219                         149.216987                      226.540912      27.666667      22.713103                    59644                         71.860241          4         84               577065              636709           1.757940       8.127780                        56.695673                      53.396613                   81.066624\n",
      "            ministral-8b-instruct-2410           30               0               30      0                0.000000                 0.000000              282                 56               34         38.766667              0.773854          38.600000               1.714039                      0.166667                                   1.388889                   45.351787                 15.281205                   453.517871                 152.812049              606.329920                           568.294733                         291.704107                      524.116634       9.400000       6.911260                    20336                         72.113475          2         26               179727              200063           0.564501       2.473163                       203.361658                     104.384974                  187.552729\n",
      "        mistral-nemo-12b-instruct-2407           30               0               30      0                0.000000                 0.000000              202                 79               11         38.766667              0.678911          38.666667               1.647011                      0.100000                                   1.375000                   73.394841                  4.250000                   733.948413                  42.500000              776.448413                           584.665140                         105.915775                      542.847323       6.733333       5.132273                     9635                         47.698020          2         20               127958              137593           0.509115       1.836560                       209.219733                      37.901473                  194.255420\n",
      "mistral-small-24b-instruct-2501@q4_k_m           42               0               42      0                0.000000                 0.000000              854                  3              123         37.404762              4.006455          37.738095               3.554863                     -0.333333                                  -0.429978                    0.488400                 22.171667                     4.884005                 221.716667              226.600672                            26.251699                         134.201175                      131.538517      20.333333      16.732715                    94750                        110.948478          6         82               540795              635545           0.919920       5.060551                         7.939421                      40.587074                   39.781794\n",
      "           mistral-small-instruct-2409           30               0               30      0                0.000000                 0.000000              272                 58               32         38.866667              0.345746          38.833333               0.592093                      0.033333                                   0.431818                   36.549603                 12.791847                   365.496032                 127.918470              493.414502                           440.127466                         172.885223                      381.098770       9.066667       5.323554                    24002                         88.242647          2         22               216674              240676           0.175392       1.905009                       157.497591                      61.866182                  136.374443\n",
      "                    mixtral-8x7b-32768           20               0               20      0                0.000000                 0.000000               40                 60                0         39.000000              0.000000          39.000000               0.000000                      0.000000                                   0.000000                  150.000000                  0.000000                  1500.000000                   0.000000             1500.000000                             0.000000                           0.000000                        0.000000       2.000000       0.000000                      660                         16.500000          2          2                10320               10980           0.000000       0.000000                         0.000000                       0.000000                    0.000000\n",
      "                    o1-mini-2024-09-12           30               9                6     15               30.000000                50.000000             4282                  2                8         17.666667             12.208910           7.033333              11.330409                     10.633333                                   9.487098                    0.094508                  0.334929                     0.945083                   3.349288                4.294371                             3.685603                           9.785673                       11.991503     142.733333      61.450812                  5228905                       1221.136151         28        200              1020768             6249673           3.159683      21.989891                         1.318876                       3.501758                    4.291104\n",
      "                 o1-preview-2024-09-12           30              14                3     13               46.666667                43.333333             3744                  4               10         17.800000             14.641462           4.033333               8.066462                     13.766667                                  18.250560                    0.292315                  0.636405                     2.923152                   6.364051                9.287203                             9.699130                          19.139350                       28.530703     124.800000      61.522858                  9959309                       2660.071848         20        200              1184284            11143593           4.335777      22.015672                         3.470789                       6.848928                   10.209581\n",
      "                                 phi-4           30               0               30      0                0.000000                 0.000000              232                 44               46         39.000000              0.000000          39.000000               0.000000                      0.000000                                   0.000000                   26.484848                 23.267316                   264.848485                 232.673160              497.521645                           286.038611                         248.578985                      216.600283       7.733333       4.630732                    77382                        333.543103          4         22               236743              314125           0.000000       1.657086                       102.357602                      88.952847                   77.509416\n",
      "                   qwen-max-2025-01-25           13               0                0     13                0.000000               100.000000             2600                  0                0         16.230769              5.418392          21.000000               8.698659                     -4.769231                                  -2.384615                    0.000000                  0.000000                     0.000000                   0.000000                0.000000                             0.000000                           0.000000                        0.000000     200.000000       0.000000                    15905                          6.117308        200        200              1041952             1057857           4.883154       0.000000                         0.000000                       0.000000                    0.000000\n",
      " qwen2.5-14b-instruct-1m@q4_k_m@q4_k_m            3               0                1      2                0.000000                66.666667              500                  0                3         16.333333             10.066446          27.000000               8.000000                    -10.666667                                  -6.666667                    0.000000                  1.000000                     0.000000                  10.000000               10.000000                             0.000000                          17.320508                       17.320508     166.666667      57.735027                   107395                        214.790000        100        200               308071              415466           9.422507      65.333333                         0.000000                      19.600000                   19.600000\n",
      "             qwen2.5-14b-instruct@q8_0           30               0               30      0                0.000000                 0.000000              398                 29               59         38.566667              1.040004          38.400000               1.904622                      0.166667                                   0.319264                   15.693122                 21.267376                   156.931217                 212.673761              369.604978                           267.142618                         200.525842                      306.563633      13.266667      10.027319                    59951                        150.630653          2         42               303196              363147           0.637945       3.588230                        95.595758                      71.757251                  109.702387\n",
      "                  qwen2.5-72b-instruct           10               0               10      0                0.000000                 0.000000              446                  4               26         34.600000              6.345602          34.400000               5.719363                      0.200000                                   0.474514                    1.323764                 12.869945                    13.237640                 128.699454              141.937094                            26.669048                         145.062094                      146.355387      44.600000      33.559897                   103378                        231.789238          6        100               304144              407522           2.829771      20.800639                        16.529647                      89.910417                   90.712009\n",
      "               qwen2.5-vl-72b-instruct           10               0               10      0                0.000000                 0.000000              244                  1               26         35.900000              5.258855          38.300000               1.567021                     -2.400000                                  -5.565176                    0.714286                 14.419192                     7.142857                 144.191919              151.334776                            22.587698                         110.075216                      112.274531      24.400000      15.966632                    88102                        361.073770          8         54               208793              296895           2.377272       9.896221                        14.000000                      68.225325                   69.588475\n",
      "                qwq-32b-preview@q4_k_m           30               0               30      0                0.000000                 0.000000              239                 44               17         38.866667              0.571346          38.966667               0.182574                     -0.100000                                  -1.306818                   30.863817                  9.735570                   308.638167                  97.355700              405.993867                           410.368000                         197.681270                      439.585866       7.966667       4.845783                   695012                       2908.000000          2         22              1406225             2101237           0.217364       1.734041                       146.848303                      70.739334                  157.303782\n",
      "                    sky-t1-32b-preview           30               0               30      0                0.000000                 0.000000              415                  9               59         38.766667              0.773854          38.800000               0.761124                     -0.033333                                  -0.080860                    2.457071                 18.510432                    24.570707                 185.104316              209.675023                            66.269383                         173.926697                      169.607361      13.833333       9.299920                   504706                       1216.159036          2         46               932760             1437466           0.345072       3.327934                        23.714194                      62.238869                   60.693215\n"
     ]
    }
   ],
   "source": [
    "csv_file_path = \"aggr.csv\"\n",
    "# CSV cols:\n",
    "# model_name,total_games,black_llm_wins,white_rand_wins,draws,black_llm_wins_percent,black_llm_draws_percent,llm_total_moves,llm_wrong_actions,llm_wrong_moves,llm_avg_material,llm_std_dev_material,rand_avg_material,rand_std_dev_material,material_diff_llm_minus_rand,material_diff_llm_minus_rand_per_100moves,wrong_actions_per_100moves,wrong_moves_per_100moves,wrong_actions_per_1000moves,wrong_moves_per_1000moves,mistakes_per_1000moves,std_dev_wrong_actions_per_1000moves,std_dev_wrong_moves_per_1000moves,std_dev_mistakes_per_1000moves,average_moves,std_dev_moves,completion_tokens_black,completion_tokens_black_per_move,min_moves,max_moves,prompt_tokens_black,total_tokens_black,moe_material_diff,moe_avg_moves,moe_wrong_actions_per_1000moves,moe_wrong_moves_per_1000moves,moe_mistakes_per_1000moves\n",
    "\n",
    "df_aggr = pd.read_csv(csv_file_path)\n",
    "print(df_aggr.to_string(index=False))\n",
    "\n",
    "# selected_columns = df_aggr[[\"model_name\", \"total_games\", \"wrong_actions_per_100moves\", \"wrong_moves_per_100moves\", \"min_moves\", \"max_moves\", \"average_moves\", \"std_dev_moves\"]]\n",
    "\n",
    "# Print the DataFrame as a properly tabbed table with headers\n",
    "# print(selected_columns.to_string(index=False))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Flattened (Plain) CSV with Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>model</th>\n",
       "      <th>time_started</th>\n",
       "      <th>winner</th>\n",
       "      <th>reason</th>\n",
       "      <th>number_of_moves</th>\n",
       "      <th>player_white_name</th>\n",
       "      <th>player_white_wrong_moves</th>\n",
       "      <th>player_white_wrong_actions</th>\n",
       "      <th>player_white_reflections_used</th>\n",
       "      <th>...</th>\n",
       "      <th>material_count_black</th>\n",
       "      <th>player_black_name</th>\n",
       "      <th>player_black_wrong_moves</th>\n",
       "      <th>player_black_wrong_actions</th>\n",
       "      <th>player_black_reflections_used</th>\n",
       "      <th>player_black_reflections_used_before_board</th>\n",
       "      <th>player_black_model</th>\n",
       "      <th>black_model_prompt_tokens</th>\n",
       "      <th>black_model_completion_tokens</th>\n",
       "      <th>black_model_total_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../_logs/no_reflection/2025-01-19_anthropic.cl...</td>\n",
       "      <td>anthropic.claude-v3-5-sonnet-v2</td>\n",
       "      <td>2025.01.19_17:52</td>\n",
       "      <td>NONE</td>\n",
       "      <td>Max moves reached</td>\n",
       "      <td>200</td>\n",
       "      <td>Random_Player</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>Player_Black</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>anthropic.claude-v3-5-sonnet-v2</td>\n",
       "      <td>113375</td>\n",
       "      <td>18006</td>\n",
       "      <td>131381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../_logs/no_reflection/2025-01-19_anthropic.cl...</td>\n",
       "      <td>anthropic.claude-v3-5-sonnet-v2</td>\n",
       "      <td>2025.01.19_15:59</td>\n",
       "      <td>NONE</td>\n",
       "      <td>Max moves reached</td>\n",
       "      <td>200</td>\n",
       "      <td>Random_Player</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>Player_Black</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>anthropic.claude-v3-5-sonnet-v2</td>\n",
       "      <td>119092</td>\n",
       "      <td>18737</td>\n",
       "      <td>137829</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows  23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                path  \\\n",
       "0  ../_logs/no_reflection/2025-01-19_anthropic.cl...   \n",
       "1  ../_logs/no_reflection/2025-01-19_anthropic.cl...   \n",
       "\n",
       "                             model      time_started winner  \\\n",
       "0  anthropic.claude-v3-5-sonnet-v2  2025.01.19_17:52   NONE   \n",
       "1  anthropic.claude-v3-5-sonnet-v2  2025.01.19_15:59   NONE   \n",
       "\n",
       "              reason  number_of_moves player_white_name  \\\n",
       "0  Max moves reached              200     Random_Player   \n",
       "1  Max moves reached              200     Random_Player   \n",
       "\n",
       "   player_white_wrong_moves  player_white_wrong_actions  \\\n",
       "0                         0                           0   \n",
       "1                         0                           0   \n",
       "\n",
       "   player_white_reflections_used  ...  material_count_black  \\\n",
       "0                              0  ...                    11   \n",
       "1                              0  ...                     8   \n",
       "\n",
       "   player_black_name  player_black_wrong_moves  player_black_wrong_actions  \\\n",
       "0       Player_Black                         1                           0   \n",
       "1       Player_Black                         0                           0   \n",
       "\n",
       "  player_black_reflections_used  player_black_reflections_used_before_board  \\\n",
       "0                             0                                           0   \n",
       "1                             0                                           0   \n",
       "\n",
       "                player_black_model  black_model_prompt_tokens  \\\n",
       "0  anthropic.claude-v3-5-sonnet-v2                     113375   \n",
       "1  anthropic.claude-v3-5-sonnet-v2                     119092   \n",
       "\n",
       "   black_model_completion_tokens black_model_total_tokens  \n",
       "0                          18006                   131381  \n",
       "1                          18737                   137829  \n",
       "\n",
       "[2 rows x 23 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def process_logs_to_dataframe(logs_path, output_csv, model_dict):\n",
    "    \"\"\"\n",
    "    Process logs into a DataFrame, substitute model names, and return the DataFrame.\n",
    "\n",
    "    Args:\n",
    "        logs_path (str): Path to the logs directory.\n",
    "        output_csv (str): Path to save the intermediate CSV file.\n",
    "        model_dict (dict): Dictionary for substituting model names.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Processed DataFrame with substituted model names.\n",
    "    \"\"\"\n",
    "    # Aggregate logs into a CSV\n",
    "    aggregate_logs_to_csv(logs_path, output_csv)\n",
    "\n",
    "    # Read the aggregated CSV into a DataFrame\n",
    "    df_plain = pd.read_csv(output_csv)\n",
    "\n",
    "    # Insert the 'model' column based on 'player_black_model'\n",
    "    df_plain.insert(df_plain.columns.get_loc(\"path\") + 1, \"model\", df_plain[\"player_black_model\"])\n",
    "\n",
    "    # Replace model names in the DataFrame using model_dict values\n",
    "    def substitute_model_names(df, model_dict):\n",
    "        def get_correct_model_name(row):\n",
    "            key = next((k for k in model_dict if os.path.dirname(row.path).endswith(k)), None)\n",
    "            return model_dict[key] if key else row[\"model\"]  # Default to the original model if no match is found\n",
    "\n",
    "        df[\"model\"] = df.apply(get_correct_model_name, axis=1)\n",
    "\n",
    "    # Apply the substitution logic\n",
    "    substitute_model_names(df_plain, model_dict)\n",
    "\n",
    "    return df_plain\n",
    "\n",
    "# Example usage\n",
    "df_plain = process_logs_to_dataframe(\"../_logs/no_reflection\", \"plain.csv\", MODEL_OVERRIDES)\n",
    "display(df_plain.head(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Aggr to Aggr-from-Plain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of total_games in df_aggr: 1944\n",
      "Sum of total_games in aggregates: 1944\n",
      "Columns in df_aggr:\n",
      "['model_name', 'total_games', 'black_llm_wins', 'white_rand_wins', 'draws', 'black_llm_wins_percent', 'black_llm_draws_percent', 'llm_total_moves', 'llm_wrong_actions', 'llm_wrong_moves', 'llm_avg_material', 'llm_std_dev_material', 'rand_avg_material', 'rand_std_dev_material', 'material_diff_llm_minus_rand', 'material_diff_llm_minus_rand_per_100moves', 'wrong_actions_per_100moves', 'wrong_moves_per_100moves', 'wrong_actions_per_1000moves', 'wrong_moves_per_1000moves', 'mistakes_per_1000moves', 'std_dev_wrong_actions_per_1000moves', 'std_dev_wrong_moves_per_1000moves', 'std_dev_mistakes_per_1000moves', 'average_moves', 'std_dev_moves', 'completion_tokens_black', 'completion_tokens_black_per_move', 'min_moves', 'max_moves', 'prompt_tokens_black', 'total_tokens_black', 'moe_material_diff', 'moe_avg_moves', 'moe_wrong_actions_per_1000moves', 'moe_wrong_moves_per_1000moves', 'moe_mistakes_per_1000moves']\n",
      "Columns in aggregates_from_plain:\n",
      "['model', 'total_games', 'black_llm_wins', 'white_rand_wins', 'draws', 'black_llm_wins_percent', 'black_llm_draws_percent', 'sum_wrong_actions', 'sum_wrong_moves', 'sum_moves', 'min_moves', 'max_moves', 'average_moves', 'std_dev_moves', 'moe_moves', 'average_material_count_white', 'std_dev_material_count_white', 'moe_material_count_white', 'average_material_count_black', 'std_dev_material_count_black', 'moe_material_count_black', 'black_model_prompt_tokens', 'average_black_model_prompt_tokens', 'std_dev_black_model_prompt_tokens', 'moe_black_model_prompt_tokens', 'black_model_completion_tokens', 'average_black_model_completion_tokens', 'std_dev_black_model_completion_tokens', 'moe_black_model_completion_tokens', 'black_model_total_tokens', 'average_black_model_total_tokens', 'std_dev_black_model_total_tokens', 'moe_black_model_total_tokens', 'material_diff_llm_minus_rand', 'std_dev_material_diff_llm_minus_rand', 'moe_material_diff_llm_minus_rand', 'material_diff_llm_minus_rand_per_100moves', 'average_wrong_actions_per_1000moves', 'std_dev_wrong_actions_per_1000moves', 'moe_wrong_actions_per_1000moves', 'average_wrong_moves_per_1000moves', 'std_dev_wrong_moves_per_1000moves', 'moe_wrong_moves_per_1000moves', 'average_mistakes_per_1000moves', 'std_dev_mistakes_per_1000moves', 'moe_mistakes_per_1000moves']\n"
     ]
    }
   ],
   "source": [
    "# Compare aggregates from aggr.csv to thoses ones obtained from plain.csv, check number of games/logs to to match (the number of log files == sum of total games)\n",
    "\n",
    "# df_plain columns\n",
    "# path,time_started,winner,reason,number_of_moves,player_white_name,player_white_wrong_moves,player_white_wrong_actions,player_white_reflections_used,player_white_reflections_used_before_board,player_white_model,material_count_white,material_count_black,player_black_name,player_black_wrong_moves,player_black_wrong_actions,player_black_reflections_used,player_black_reflections_used_before_board,player_black_model,black_model_prompt_tokens,black_model_completion_tokens,black_model_total_tokens\n",
    "\n",
    "\n",
    "# Group the data by 'player_black_model' and calculate the number of moves for each model\n",
    "grouped_data = df_plain.groupby('model')['number_of_moves']\n",
    "\n",
    "# Calculate per-game material difference\n",
    "df_plain['material_diff_per_game'] = df_plain['material_count_black'] - df_plain['material_count_white']\n",
    "\n",
    "# Calculate per-game material difference per 100 moves\n",
    "df_plain['material_diff_per_100moves'] = df_plain.apply(\n",
    "    lambda row: (row['material_diff_per_game'] / row['number_of_moves'] * 100) if row['number_of_moves'] > 0 else 0,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Calculate per-game rates for wrong actions, wrong moves, and mistakes\n",
    "df_plain['wrong_actions_per_1000moves'] = df_plain.apply(\n",
    "    lambda row: (row['player_black_wrong_actions'] / row['number_of_moves'] * 1000) if row['number_of_moves'] > 0 else 0,\n",
    "    axis=1\n",
    ")\n",
    "df_plain['wrong_moves_per_1000moves'] = df_plain.apply(\n",
    "    lambda row: (row['player_black_wrong_moves'] / row['number_of_moves'] * 1000) if row['number_of_moves'] > 0 else 0,\n",
    "    axis=1\n",
    ")\n",
    "df_plain['mistakes_per_1000moves'] = df_plain.apply(\n",
    "    lambda row: ((row['player_black_wrong_actions'] + row['player_black_wrong_moves']) / row['number_of_moves'] * 1000) if row['number_of_moves'] > 0 else 0,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Perform aggregation\n",
    "aggregates_from_plain = df_plain.groupby('model').agg(\n",
    "    total_games=('number_of_moves', 'count'),\n",
    "    black_llm_wins=('winner', lambda x: (x == 'Player_Black').sum()),\n",
    "    white_rand_wins=('winner', lambda x: (x == 'Random_Player').sum()),\n",
    "    draws=('winner', lambda x: (x == 'NONE').sum()),\n",
    "    black_llm_wins_percent=('winner', lambda x: (x == 'Player_Black').sum() / len(x) * 100),\n",
    "    black_llm_draws_percent=('winner', lambda x: (x == 'NONE').sum() / len(x) * 100),\n",
    "    sum_wrong_actions=('player_black_wrong_actions', 'sum'),\n",
    "    sum_wrong_moves=('player_black_wrong_moves', 'sum'),\n",
    "    sum_moves=('number_of_moves', 'sum'),\n",
    "    min_moves=('number_of_moves', 'min'),\n",
    "    max_moves=('number_of_moves', 'max'),\n",
    "    average_moves=('number_of_moves', 'mean'),\n",
    "    std_dev_moves=('number_of_moves', lambda x: x.std(ddof=1)),\n",
    "    moe_moves=('number_of_moves', lambda x: 1.96 * (x.std(ddof=1) / (len(x) ** 0.5))),\n",
    "    average_material_count_white=('material_count_white', 'mean'),\n",
    "    std_dev_material_count_white=('material_count_white', lambda x: x.std(ddof=1)),\n",
    "    moe_material_count_white=('material_count_white', lambda x: 1.96 * (x.std(ddof=1) / (len(x) ** 0.5))),\n",
    "    average_material_count_black=('material_count_black', 'mean'),\n",
    "    std_dev_material_count_black=('material_count_black', lambda x: x.std(ddof=1)),\n",
    "    moe_material_count_black=('material_count_black', lambda x: 1.96 * (x.std(ddof=1) / (len(x) ** 0.5))),\n",
    "    black_model_prompt_tokens=('black_model_prompt_tokens', 'sum'),\n",
    "    average_black_model_prompt_tokens=('black_model_prompt_tokens', 'mean'),\n",
    "    std_dev_black_model_prompt_tokens=('black_model_prompt_tokens', lambda x: x.std(ddof=1)),\n",
    "    moe_black_model_prompt_tokens=('black_model_prompt_tokens', lambda x: 1.96 * (x.std(ddof=1) / (len(x) ** 0.5))),\n",
    "    black_model_completion_tokens=('black_model_completion_tokens', 'sum'),\n",
    "    average_black_model_completion_tokens=('black_model_completion_tokens', 'mean'),\n",
    "    std_dev_black_model_completion_tokens=('black_model_completion_tokens', lambda x: x.std(ddof=1)),\n",
    "    moe_black_model_completion_tokens=('black_model_completion_tokens', lambda x: 1.96 * (x.std(ddof=1) / (len(x) ** 0.5))),\n",
    "    black_model_total_tokens=('black_model_total_tokens', 'sum'),\n",
    "    average_black_model_total_tokens=('black_model_total_tokens', 'mean'),\n",
    "    std_dev_black_model_total_tokens=('black_model_total_tokens', lambda x: x.std(ddof=1)),\n",
    "    moe_black_model_total_tokens=('black_model_total_tokens', lambda x: 1.96 * (x.std(ddof=1) / (len(x) ** 0.5))),\n",
    "    material_diff_llm_minus_rand=('material_diff_per_game', 'mean'),\n",
    "    std_dev_material_diff_llm_minus_rand=('material_diff_per_game', lambda x: x.std(ddof=1)),\n",
    "    moe_material_diff_llm_minus_rand=('material_diff_per_game', lambda x: 1.96 * (x.std(ddof=1) / (len(x) ** 0.5))),\n",
    "    material_diff_llm_minus_rand_per_100moves=('material_diff_per_100moves', 'mean'),\n",
    "    average_wrong_actions_per_1000moves=('wrong_actions_per_1000moves', 'mean'),\n",
    "    std_dev_wrong_actions_per_1000moves=('wrong_actions_per_1000moves', lambda x: x.std(ddof=1)),\n",
    "    moe_wrong_actions_per_1000moves=('wrong_actions_per_1000moves', lambda x: 1.96 * (x.std(ddof=1) / (len(x) ** 0.5))),\n",
    "    average_wrong_moves_per_1000moves=('wrong_moves_per_1000moves', 'mean'),\n",
    "    std_dev_wrong_moves_per_1000moves=('wrong_moves_per_1000moves', lambda x: x.std(ddof=1)),\n",
    "    moe_wrong_moves_per_1000moves=('wrong_moves_per_1000moves', lambda x: 1.96 * (x.std(ddof=1) / (len(x) ** 0.5))),\n",
    "    average_mistakes_per_1000moves=('mistakes_per_1000moves', 'mean'),\n",
    "    std_dev_mistakes_per_1000moves=('mistakes_per_1000moves', lambda x: x.std(ddof=1)),\n",
    "    moe_mistakes_per_1000moves=('mistakes_per_1000moves', lambda x: 1.96 * (x.std(ddof=1) / (len(x) ** 0.5)))\n",
    ").reset_index()\n",
    "\n",
    "# Calculate and print the sum of total_games in df_aggr\n",
    "df_aggr_total_games_sum = df_aggr[\"total_games\"].sum()\n",
    "print(f\"Sum of total_games in df_aggr: {df_aggr_total_games_sum}\")\n",
    "\n",
    "# Calculate and print the sum of total_games in aggregates\n",
    "aggregates_total_games_sum = aggregates_from_plain[\"total_games\"].sum()\n",
    "print(f\"Sum of total_games in aggregates: {aggregates_total_games_sum}\")\n",
    "\n",
    "# Print column names from df_aggr\n",
    "print(\"Columns in df_aggr:\")\n",
    "print(df_aggr.columns.tolist())\n",
    "\n",
    "# Print column names from aggregates\n",
    "print(\"Columns in aggregates_from_plain:\")\n",
    "print(aggregates_from_plain.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the calculated aggregates\n",
    "# print(aggregates_from_plain.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 'deepseek-r1-distill-qwen-32b@q4_k_m' not found in aggregates_from_plain.\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary to map the best matches between df_aggr and aggregates_from_plain column names\n",
    "column_mapping = {\n",
    "    'model_name': 'model',\n",
    "    'total_games': 'total_games',\n",
    "    'black_llm_wins': 'black_llm_wins',\n",
    "    'white_rand_wins': 'white_rand_wins',\n",
    "    'draws': 'draws',\n",
    "    'black_llm_wins_percent': 'black_llm_wins_percent',\n",
    "    'black_llm_draws_percent': 'black_llm_draws_percent',\n",
    "    'llm_total_moves': 'sum_moves',\n",
    "    'llm_wrong_actions': 'sum_wrong_actions',\n",
    "    'llm_wrong_moves': 'sum_wrong_moves',\n",
    "    'llm_avg_material': 'average_material_count_black',\n",
    "    'llm_std_dev_material': 'std_dev_material_count_black',\n",
    "    'rand_avg_material': 'average_material_count_white',\n",
    "    'rand_std_dev_material': 'std_dev_material_count_white',\n",
    "    'material_diff_llm_minus_rand': 'material_diff_llm_minus_rand',\n",
    "    'material_diff_llm_minus_rand_per_100moves': 'material_diff_llm_minus_rand_per_100moves',\n",
    "    'wrong_actions_per_1000moves': 'average_wrong_actions_per_1000moves',\n",
    "    'wrong_moves_per_1000moves': 'average_wrong_moves_per_1000moves', \n",
    "    'mistakes_per_1000moves': 'average_mistakes_per_1000moves',\n",
    "    'std_dev_wrong_actions_per_1000moves': 'std_dev_wrong_actions_per_1000moves',\n",
    "    'std_dev_wrong_moves_per_1000moves': 'std_dev_wrong_moves_per_1000moves',\n",
    "    'std_dev_mistakes_per_1000moves': 'std_dev_mistakes_per_1000moves',\n",
    "    'average_moves': 'average_moves',\n",
    "    'std_dev_moves': 'std_dev_moves',\n",
    "    'completion_tokens_black': 'black_model_completion_tokens',\n",
    "    'min_moves': 'min_moves',\n",
    "    'max_moves': 'max_moves',\n",
    "    'prompt_tokens_black': 'black_model_prompt_tokens',\n",
    "    'total_tokens_black': 'black_model_total_tokens',\n",
    "    'moe_material_diff': 'moe_material_diff_llm_minus_rand',\n",
    "    'moe_avg_moves': 'moe_moves',\n",
    "    'moe_wrong_actions_per_1000moves': 'moe_wrong_actions_per_1000moves',\n",
    "    'moe_wrong_moves_per_1000moves': 'moe_wrong_moves_per_1000moves',\n",
    "    'moe_mistakes_per_1000moves': 'moe_mistakes_per_1000moves',\n",
    "}\n",
    "\n",
    "# Iterate over the rows in df_aggr\n",
    "for index, row in df_aggr.iterrows():\n",
    "    model_name = row['model_name']\n",
    "    \n",
    "    # Find the corresponding row in aggregates_from_plain\n",
    "    matching_row = aggregates_from_plain[aggregates_from_plain['model'] == model_name]\n",
    "    \n",
    "    if matching_row.empty:\n",
    "        print(f\"Model '{model_name}' not found in aggregates_from_plain.\")\n",
    "        continue\n",
    "    \n",
    "    # Compare the values of mapped columns\n",
    "    for df_aggr_col, aggregates_col in column_mapping.items():\n",
    "        if aggregates_col is None:\n",
    "            # Skip columns that have no mapping\n",
    "            continue\n",
    "        \n",
    "        df_aggr_value = row[df_aggr_col]\n",
    "        \n",
    "        try:\n",
    "            # Safely access the value in matching_row\n",
    "            aggregates_value = matching_row.iloc[0].get(aggregates_col, None)\n",
    "        except KeyError:\n",
    "            print(f\"Column '{aggregates_col}' not found in aggregates_from_plain for model '{model_name}'.\")\n",
    "            continue\n",
    "        \n",
    "        # Ensure both values are converted to numeric if possible\n",
    "        try:\n",
    "            df_aggr_value = pd.to_numeric(df_aggr_value, errors='coerce')\n",
    "            aggregates_value = pd.to_numeric(aggregates_value, errors='coerce')\n",
    "        except Exception as e:\n",
    "            print(f\"Error converting values to numeric for column '{df_aggr_col}': {e}\")\n",
    "            continue\n",
    "        \n",
    "        if not pd.isna(df_aggr_value) and not pd.isna(aggregates_value):\n",
    "            if not np.isclose(df_aggr_value, aggregates_value, atol=1e-6):\n",
    "                print(f\"Discrepancy for model '{model_name}' in column '{df_aggr_col}':\")\n",
    "                print(f\"  df_aggr value: {df_aggr_value}\")\n",
    "                print(f\"  aggregates_from_plain value: {aggregates_value}\")\n",
    "        elif pd.isna(df_aggr_value) != pd.isna(aggregates_value):\n",
    "            print(f\"Discrepancy for model '{model_name}' in column '{df_aggr_col}':\")\n",
    "            print(f\"  df_aggr value: {df_aggr_value}\")\n",
    "            print(f\"  aggregates_from_plain value: {aggregates_value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the rows in df_aggr for the specified model\n",
    "df_aggr_filtered = df_aggr[df_aggr['model_name'] == 'deepseek-r1-distill-qwen-32b']\n",
    "\n",
    "# Filter the rows in aggregates_from_plain for the specified model\n",
    "aggregates_filtered = aggregates_from_plain[aggregates_from_plain['model'] == 'gemini-2.0-flash-thinking-exp-01-21']\n",
    "\n",
    "# Print the first 2 rows from each dataset\n",
    "print(\"Rows from df_aggr:\")\n",
    "print(df_aggr_filtered.head(1).to_string())\n",
    "\n",
    "print(\"\\nRows from aggregates_from_plain:\")\n",
    "print(aggregates_filtered.head(1).to_string())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
